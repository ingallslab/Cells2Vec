{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd CellModeller-ingallslab\n",
    "%pip install -e . --use-pep517\n",
    "#%cd CellProfilerAnalysis/\n",
    "#%pip install - e . --use-pep517\n",
    "#%pip install CellProfiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_numpy_array(pickle_to_dict):\n",
    "    # Definition of dictionary\n",
    "    property_dict = {'time': [], 'id': [], 'parent': [], 'label': [],\n",
    "                     'cellType': [], 'divideFlag': [], 'cellAge': [], 'growthRate': [], 'LifeHistory': [],\n",
    "                     'startVol': [], 'targetVol': [], 'pos': [], 'radius': [], 'length': [], 'dir': [],\n",
    "                     'ends0': [], 'ends1': [], 'strainRate': [], 'strainRate_rolling': []}\n",
    "\n",
    "    # Fill the dictionary\n",
    "    for key in pickle_to_dict['cellStates'].keys():\n",
    "        cell_state = pickle_to_dict['cellStates'][key]\n",
    "\n",
    "        # Append values to dictionary, assign zero if attribute not present\n",
    "        property_dict['time'].append(cell_state.time)\n",
    "        property_dict['id'].append(cell_state.id)\n",
    "        property_dict['label'].append(cell_state.label)\n",
    "        property_dict['cellType'].append(cell_state.cellType)\n",
    "        property_dict['divideFlag'].append(cell_state.divideFlag)\n",
    "        property_dict['cellAge'].append(cell_state.cellAge)\n",
    "        property_dict['growthRate'].append(cell_state.growthRate)\n",
    "\n",
    "        # Handle 'LifeHistory' attribute\n",
    "        # If not present, assign value 0\n",
    "        if hasattr(cell_state, 'LifeHistory'):\n",
    "            property_dict['LifeHistory'].append(cell_state.LifeHistory)\n",
    "        else:\n",
    "            property_dict['LifeHistory'].append(0)\n",
    "\n",
    "        property_dict['startVol'].append(cell_state.startVol)\n",
    "        property_dict['targetVol'].append(cell_state.targetVol)\n",
    "        property_dict['pos'].append(\n",
    "            np.sqrt(np.sum(np.power(cell_state.pos, 2))))\n",
    "        property_dict['radius'].append(cell_state.radius)\n",
    "        property_dict['length'].append(cell_state.length)\n",
    "        property_dict['dir'].append(np.arctan2(\n",
    "            cell_state.dir[1], cell_state.dir[0]))\n",
    "        property_dict['ends0'].append(\n",
    "            np.sqrt(np.sum(np.power(cell_state.ends[0], 2))))\n",
    "        property_dict['ends1'].append(\n",
    "            np.sqrt(np.sum(np.power(cell_state.ends[1], 2))))\n",
    "        property_dict['strainRate'].append(cell_state.strainRate)\n",
    "        property_dict['strainRate_rolling'].append(\n",
    "            cell_state.strainRate_rolling)\n",
    "\n",
    "    # Structure of 'lineage': id : parent id\n",
    "    # If no parent, assign value 0\n",
    "    for bac_id in property_dict['id']:\n",
    "        if bac_id in pickle_to_dict['lineage']:\n",
    "            property_dict['parent'].append(pickle_to_dict['lineage'][bac_id])\n",
    "        else:\n",
    "            property_dict['parent'].append(0)\n",
    "\n",
    "    # Convert dictionary to pandas DataFrame\n",
    "    df_bacteria = pd.DataFrame.from_dict(property_dict)\n",
    "\n",
    "    # Replacing NaN values with 0\n",
    "    df_bacteria.fillna(0, inplace=True)\n",
    "\n",
    "    # Convert all columns to float\n",
    "    df_bacteria = df_bacteria.astype(float)\n",
    "\n",
    "    return df_bacteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 19])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "pickle_file = '/home/stormageddon/MITACS/test/why/adh_0.1_27/step-00120.pickle'\n",
    "\n",
    "\n",
    "pickle_to_dict = np.load(pickle_file, allow_pickle=True)\n",
    "df = make_numpy_array(pickle_to_dict)\n",
    "\n",
    "# Convert DataFrame to PyTorch tensor\n",
    "tensor = torch.tensor(df.values)\n",
    "\n",
    "# write to csv\n",
    "#df.to_csv('input features.csv', index=False)\n",
    "\n",
    "print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([873, 19])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing the pickle files\n",
    "directory = '/home/stormageddon/MITACS/test/why/adh_0.01_27/'\n",
    "\n",
    "# Get the list of pickle files in the directory\n",
    "pickle_files = [file for file in os.listdir(\n",
    "    directory) if file.endswith('.pickle')]\n",
    "\n",
    "# Initialize an empty list to store the tensor for each pickle file\n",
    "tensor_list = []\n",
    "\n",
    "# Process each pickle file\n",
    "for pickle_file in pickle_files:\n",
    "    # Load the pickle file and convert it to a dictionary\n",
    "    pickle_to_dict = np.load(os.path.join(\n",
    "        directory, pickle_file), allow_pickle=True)\n",
    "    # Assuming you have a function to convert the dictionary to a DataFrame\n",
    "    df = make_numpy_array(pickle_to_dict)\n",
    "\n",
    "    # Convert DataFrame to PyTorch tensor\n",
    "    tensor = torch.tensor(df.values)\n",
    "\n",
    "    # Append the tensor to the list\n",
    "    tensor_list.append(tensor)\n",
    "\n",
    "# Concatenate the tensors along the first dimension (num_pickles)\n",
    "final_tensor = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "print(final_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([879, 19])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing the pickle files\n",
    "directory = '/home/stormageddon/MITACS/test/why/adh_0.01_28/'\n",
    "\n",
    "# Get the list of pickle files in the directory\n",
    "pickle_files = [file for file in os.listdir(\n",
    "    directory) if file.endswith('.pickle')]\n",
    "\n",
    "# Initialize an empty list to store the tensor for each pickle file\n",
    "tensor_list = []\n",
    "\n",
    "# Process each pickle file\n",
    "for pickle_file in pickle_files:\n",
    "    # Load the pickle file and convert it to a dictionary\n",
    "    pickle_to_dict = np.load(os.path.join(\n",
    "        directory, pickle_file), allow_pickle=True)\n",
    "    # Assuming you have a function to convert the dictionary to a DataFrame\n",
    "    df = make_numpy_array(pickle_to_dict)\n",
    "\n",
    "    # Convert DataFrame to PyTorch tensor\n",
    "    tensor = torch.tensor(df.values)\n",
    "\n",
    "    # Append the tensor to the list\n",
    "    tensor_list.append(tensor)\n",
    "\n",
    "# Concatenate the tensors along the first dimension (num_pickles)\n",
    "final_tensor2 = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "print(final_tensor2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1121, 19])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing the pickle files\n",
    "directory = '/home/stormageddon/MITACS/test/why/adh_0.01_24/'\n",
    "\n",
    "# Get the list of pickle files in the directory\n",
    "pickle_files = [file for file in os.listdir(\n",
    "    directory) if file.endswith('.pickle')]\n",
    "\n",
    "# Initialize an empty list to store the tensor for each pickle file\n",
    "tensor_list = []\n",
    "\n",
    "# Process each pickle file\n",
    "for pickle_file in pickle_files:\n",
    "    # Load the pickle file and convert it to a dictionary\n",
    "    pickle_to_dict = np.load(os.path.join(\n",
    "        directory, pickle_file), allow_pickle=True)\n",
    "    # Assuming you have a function to convert the dictionary to a DataFrame\n",
    "    df = make_numpy_array(pickle_to_dict)\n",
    "\n",
    "    # Convert DataFrame to PyTorch tensor\n",
    "    tensor = torch.tensor(df.values)\n",
    "\n",
    "    # Append the tensor to the list\n",
    "    tensor_list.append(tensor)\n",
    "\n",
    "# Concatenate the tensors along the first dimension (num_pickles)\n",
    "final_tensor3 = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "print(final_tensor3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_obs = max(final_tensor.shape[0], final_tensor2.shape[0], final_tensor3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tensor1 = torch.cat((final_tensor, torch.zeros(max_num_obs - final_tensor.shape[0], final_tensor.shape[1])), dim=0)\n",
    "padded_tensor2 = torch.cat((final_tensor2, torch.zeros(max_num_obs - final_tensor2.shape[0], final_tensor2.shape[1])), dim=0)\n",
    "padded_tensor3 = torch.cat((final_tensor3, torch.zeros(max_num_obs - final_tensor3.shape[0], final_tensor3.shape[1])), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1121, 19])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = torch.stack((padded_tensor1, padded_tensor2, padded_tensor3), dim=0)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Conv1d(\n",
    "            input_shape[2], 32, kernel_size=3, padding=1)\n",
    "        self.encoder_lstm = nn.LSTM(32, latent_dim, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(latent_dim, 32, batch_first=True)\n",
    "        self.decoder_cnn = nn.ConvTranspose1d(\n",
    "            32, input_shape[2], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.encoder_cnn(x.transpose(1, 2))\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        #print(x.shape)\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        #print(h.shape)\n",
    "        x = h.repeat(1, x.size(1), 1)\n",
    "        #print(x.shape)\n",
    "        x, _ = self.decoder_lstm(x)\n",
    "        #print(x.shape)\n",
    "        x = self.decoder_cnn(x.transpose(1, 2))\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        #print(x.shape)\n",
    "        x = x.view(3, 1121,19\n",
    "                   )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "array=array.float()\n",
    "losss=[]\n",
    "input_shape = (3, 1121, 19)\n",
    "latent_dim = 64\n",
    "model = Autoencoder(input_shape, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "input_tensor = torch.randn(input_shape).float()\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_tensor)\n",
    "    \n",
    "    print(outputs.dtype)\n",
    "    loss= criterion(outputs, array)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for monitoring\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5053.916417910447\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the directory containing the folders\n",
    "directory = '/home/stormageddon/MITACS/test/why'\n",
    "\n",
    "# Get the list of folders in the directory\n",
    "folders = [folder for folder in os.listdir(\n",
    "    directory) if os.path.isdir(os.path.join(directory, folder))]\n",
    "\n",
    "# Initialize an empty list to store the final tensors\n",
    "final_tensors = []\n",
    "\n",
    "# Process each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(directory, folder)\n",
    "\n",
    "    # Get the list of pickle files in the folder\n",
    "    pickle_files = [file for file in os.listdir(\n",
    "        folder_path) if file.endswith('.pickle')]\n",
    "\n",
    "    # Initialize an empty list to store the tensor for each pickle file in the folder\n",
    "    tensor_list = []\n",
    "    count_large=0\n",
    "\n",
    "    # Process each pickle file in the folder\n",
    "    for pickle_file in pickle_files:\n",
    "        # Load the pickle file and convert it to a dictionary\n",
    "        pickle_to_dict = np.load(os.path.join(\n",
    "            folder_path, pickle_file), allow_pickle=True)\n",
    "        # Assuming you have a function to convert the dictionary to a DataFrame\n",
    "        df = make_numpy_array(pickle_to_dict)\n",
    "\n",
    "        # Convert DataFrame to PyTorch tensor\n",
    "        tensor = torch.tensor(df.values)\n",
    "\n",
    "        # Append the tensor to the list\n",
    "        tensor_list.append(tensor)\n",
    "\n",
    "    # Find the maximum number of observations in tensor_list\n",
    "    max_num_obs = max([tensor.shape[0] for tensor in tensor_list])\n",
    "\n",
    "    # Pad tensors with zeros to match the maximum number of observations\n",
    "    padded_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        pad_shape = (max_num_obs - tensor.shape[0], tensor.shape[1])\n",
    "        padded_tensor = F.pad(tensor, (0, 0, 0, pad_shape[0]))\n",
    "        padded_tensors.append(padded_tensor)\n",
    "\n",
    "    # Concatenate the padded tensors along the first dimension (num_pickles) in the folder\n",
    "    final_tensor = torch.cat(padded_tensors, dim=0)\n",
    "\n",
    "    # Append the padded tensor to the list of final tensors\n",
    "    final_tensors.append(final_tensor)\n",
    "\n",
    "# Pad the final tensors in final_tensors to have the same number of rows\n",
    "max_num_rows = max([tensor.shape[0] for tensor in final_tensors])\n",
    "print(sum([tensor.shape[0] for tensor in final_tensors])/len(final_tensors))\n",
    "padded_final_tensors = []\n",
    "for tensor in final_tensors:\n",
    "    pad_shape = (max_num_rows - tensor.shape[0], tensor.shape[1])\n",
    "    padded_tensor = F.pad(tensor, (0, 0, 0, pad_shape[0]))\n",
    "    padded_final_tensors.append(padded_tensor)\n",
    "\n",
    "# Stack the final tensors along a new dimension (num_folders)\n",
    "array = torch.stack(padded_final_tensors, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([335, 6144, 19])\n",
      "Final tensor saved in compressed format.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "# Define the root directory containing the folders\n",
    "\n",
    "\n",
    "\n",
    "# Save the final tensor array in a compressed format\n",
    "output_file = '/home/stormageddon/MITACS/test/compressed_input.pt.gz'\n",
    "with gzip.open(output_file, 'wb') as f:\n",
    "    torch.save(array, f)\n",
    "\n",
    "print(array.shape)\n",
    "print(\"Final tensor saved in compressed format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, input_size)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # h_n: (1, batch_size, hidden_size)\n",
    "        features = self.fc(h_n.squeeze(0))\n",
    "        # features: (batch_size, output_size)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([335, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define the input size, hidden size, and output size\n",
    "input_size = 19  # Number of features in the input\n",
    "hidden_size = 128  # Size of the LSTM hidden state\n",
    "output_size = 64  # Size of the extracted features\n",
    "\n",
    "# Instantiate the feature extraction model\n",
    "model = FeatureExtractor(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate random input data (replace with your actual time series data)\n",
    "batch_size = 335  # Number of simulations\n",
    "sequence_length = 6144  # Number of time steps\n",
    "input_data = array.float()\n",
    "\n",
    "\n",
    "# Pass the input data through the model\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
